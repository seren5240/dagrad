{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x11752e550>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "import torch\n",
    "import dagrad.flex as flex\n",
    "from dagrad.utils import utils\n",
    "\n",
    "utils.set_random_seed(1)\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Struct Learn via Flex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`flex.struct_learn` is a function that requires the following 5 inputs:\n",
    "- `dataset`: A data matrix with `n x d` shape, where `n` is the number of samples and `d` is the number of variables/features.\n",
    "\n",
    "- `model`: The SEM module. We provide standard SEM implementations such as---LinearModel, LogisticModel, MLP\n",
    "\n",
    "- `constrained_solver`: An instance of a ConstrainedSolver class. We provide implementations such as `PathFollowing` and `AugmentedLagrangian`\n",
    "\n",
    "- `unconstrained_solver`: An instance of an UnconstrainedSolver class. We provide implementation for `GradientBasedSolver`\n",
    "\n",
    "- `loss_fn`: Instance of a Loss class. All available losses can be found at flex/loss.py\n",
    "\n",
    "- `dag_fn`: Instance of a DagFn class. All available DAG functions can be found at flex/dags.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate linear data\n",
    "n, d, s0, graph_type, sem_type = 1000, 20, 20, \"ER\", \"gauss\"\n",
    "linear_B_true = utils.simulate_dag(d, s0, graph_type)\n",
    "linear_dataset = utils.simulate_linear_sem(linear_B_true, n, sem_type)\n",
    "\n",
    "# Generate non-linear data\n",
    "n, d, s0, graph_type, sem_type = 1000, 20, 20, \"ER\", \"mlp\"\n",
    "nonlinear_B_true = utils.simulate_dag(d, s0, graph_type)\n",
    "nonlinear_dataset = utils.simulate_nonlinear_sem(nonlinear_B_true, n, sem_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using flex to implement NOTEARS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## linear NOTEARS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kbello/Dropbox/Academics/Research/Code/dagrad/dagrad/flex/modules/constrained_solvers.py:155: UserWarning: Using the first value from num_steps for the first 8 iterations\n",
      "  self.vwarn(f\"Using the first value from num_steps for the first {missing_steps} iterations\")\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "370dc4f2098149acb65dcb60353459b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Time: 122.13103890419006\n",
      "Results:  {'fdr': 0.0, 'tpr': 1.0, 'fpr': 0.0, 'shd': 0, 'nnz': 20}\n"
     ]
    }
   ],
   "source": [
    "# Linear model\n",
    "model = flex.LinearModel(d)\n",
    "\n",
    "# Use AML to solve the constrained problem\n",
    "cons_solver = flex.AugmentedLagrangian(\n",
    "    num_iter=10,\n",
    "    num_steps=[3e4,6e4],\n",
    "    l1_coeff=0.03,\n",
    ")\n",
    "\n",
    "# Use Adam to solve the unconstrained problem\n",
    "uncons_solver = flex.GradientBasedSolver(\n",
    "    optimizer=torch.optim.Adam(model.parameters(), lr=3e-4),\n",
    ")\n",
    "\n",
    "# Use MSE loss\n",
    "loss_fn = flex.MSELoss()\n",
    "\n",
    "# Use Trace of matrix exponential as DAG function\n",
    "dag_fn = flex.Exp()\n",
    "\n",
    "# Learn the DAG\n",
    "W_est = flex.struct_learn(\n",
    "    dataset=linear_dataset,\n",
    "    model=model,\n",
    "    constrained_solver=cons_solver,\n",
    "    unconstrained_solver=uncons_solver,\n",
    "    loss_fn=loss_fn,\n",
    "    dag_fn=dag_fn,\n",
    "    w_threshold=0.3,\n",
    ")\n",
    "\n",
    "acc = utils.count_accuracy(linear_B_true, W_est != 0)\n",
    "print(\"Results: \", acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## nonlinear NOTEARS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb76064568ca44528a3d7cfa463da5cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Time: 730.787192106247\n",
      "Results:  {'fdr': 0.0, 'tpr': 0.8, 'fpr': 0.0, 'shd': 4, 'nnz': 16}\n"
     ]
    }
   ],
   "source": [
    "# Nonlinear model\n",
    "model = flex.MLP(dims=[d, 10, 1], activation=\"sigmoid\", bias=True)\n",
    "\n",
    "# Use AML to solve the constrained problem\n",
    "cons_solver = flex.AugmentedLagrangian(\n",
    "    num_iter=10,\n",
    "    num_steps=[4e4,6e4],\n",
    "    l1_coeff=0.01,\n",
    "    weight_decay=0.01,\n",
    ")\n",
    "\n",
    "# Use Adam to solve the unconstrained problem\n",
    "uncons_solver = flex.GradientBasedSolver(\n",
    "    optimizer=torch.optim.Adam(model.parameters(), lr=3e-4),\n",
    ")\n",
    "\n",
    "# Use MSE loss\n",
    "loss_fn = flex.MSELoss()\n",
    "\n",
    "# Use Trace of matrix exponential as DAG function\n",
    "dag_fn = flex.Exp()\n",
    "\n",
    "# Learn the DAG\n",
    "W_est = flex.struct_learn(\n",
    "    dataset=nonlinear_dataset,\n",
    "    model=model,\n",
    "    constrained_solver=cons_solver,\n",
    "    unconstrained_solver=uncons_solver,\n",
    "    loss_fn=loss_fn,\n",
    "    dag_fn=dag_fn,\n",
    "    w_threshold=0.3,\n",
    ")\n",
    "\n",
    "acc = utils.count_accuracy(nonlinear_B_true, W_est != 0)\n",
    "print(\"Results: \", acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using flex to implement DAGMA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear DAGMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3fb9952434d441339632db631e7b9dd9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Time: 40.260650873184204\n",
      "Results:  {'fdr': 0.0, 'tpr': 1.0, 'fpr': 0.0, 'shd': 0, 'nnz': 20}\n"
     ]
    }
   ],
   "source": [
    "# Linear model\n",
    "model = flex.LinearModel(d)\n",
    "\n",
    "# Use path following to solve the constrained problem\n",
    "cons_solver = flex.PathFollowing(\n",
    "    num_iter=5,\n",
    "    mu_init=1.0,\n",
    "    mu_scale=0.1,\n",
    "    logdet_coeff=[1.0, .9, .8, .7, .6],\n",
    "    num_steps=[3e4, 6e4],\n",
    "    l1_coeff=0.03,\n",
    ")\n",
    "\n",
    "# use Adam to solve the unconstrained problem\n",
    "uncons_solver = flex.GradientBasedSolver(\n",
    "    optimizer=torch.optim.Adam(model.parameters(), lr=3e-4, betas=(0.99, 0.999))\n",
    ")\n",
    "\n",
    "# Use MSE loss\n",
    "loss_fn = flex.MSELoss()\n",
    "\n",
    "# Use LogDet as DAG function\n",
    "dag_fn = flex.LogDet()\n",
    "\n",
    "W_est = flex.struct_learn(\n",
    "    dataset=linear_dataset,\n",
    "    model=model,\n",
    "    constrained_solver=cons_solver,\n",
    "    unconstrained_solver=uncons_solver,\n",
    "    loss_fn=loss_fn,\n",
    "    dag_fn=dag_fn,\n",
    "    w_threshold=0.3,\n",
    ")\n",
    "\n",
    "acc = utils.count_accuracy(linear_B_true, W_est != 0)\n",
    "print(\"Results: \", acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nonlinear DAGMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kbello/Dropbox/Academics/Research/Code/dagrad/dagrad/flex/modules/constrained_solvers.py:49: UserWarning: Using the first value from num_steps for the first 2 iterations\n",
      "  self.vwarn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "752bbd341a6f4a08819010d6ae2737db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Time: 433.4083251953125\n",
      "Results:  {'fdr': 0.0, 'tpr': 0.85, 'fpr': 0.0, 'shd': 3, 'nnz': 17}\n"
     ]
    }
   ],
   "source": [
    "# nonlinear model\n",
    "model = flex.MLP(dims=[d, 10, 1], activation=\"sigmoid\", bias=True)\n",
    "\n",
    "# Use path following to solve the constrained problem\n",
    "cons_solver = flex.PathFollowing(\n",
    "    num_iter=4,\n",
    "    mu_init=0.1,\n",
    "    mu_scale=0.1,\n",
    "    logdet_coeff=1.0,\n",
    "    num_steps=[5e4, 8e4],\n",
    "    weight_decay=0.02,\n",
    "    l1_coeff=0.005,\n",
    ")\n",
    "\n",
    "# use Adam to solve the unconstrained problem\n",
    "uncons_solver = flex.GradientBasedSolver(\n",
    "    optimizer=torch.optim.Adam(model.parameters(), lr=2e-4, betas=(0.99, 0.999))\n",
    ")\n",
    "\n",
    "# Use NLL loss\n",
    "loss_fn = flex.NLLLoss()\n",
    "\n",
    "# Use LogDet as DAG function\n",
    "dag_fn = flex.LogDet()\n",
    "\n",
    "# Learn the DAG\n",
    "W_est = flex.struct_learn(\n",
    "    dataset=nonlinear_dataset,\n",
    "    model=model,\n",
    "    constrained_solver=cons_solver,\n",
    "    unconstrained_solver=uncons_solver,\n",
    "    loss_fn=loss_fn,\n",
    "    dag_fn=dag_fn,\n",
    "    w_threshold=0.3,\n",
    ")\n",
    "\n",
    "acc = utils.count_accuracy(nonlinear_B_true, W_est != 0)\n",
    "print(\"Results: \", acc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dagrad",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
