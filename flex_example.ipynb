{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x10a05db70>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "import torch\n",
    "import dagrad.flex as flex\n",
    "from dagrad.utils import utils\n",
    "import numpy as np\n",
    "\n",
    "utils.set_random_seed(1)\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def postprocess(B, graph_thres=0.3):\n",
    "    \"\"\"Post-process estimated solution:\n",
    "        (1) Thresholding.\n",
    "        (2) Remove the edges with smallest absolute weight until a DAG\n",
    "            is obtained.\n",
    "\n",
    "    Args:\n",
    "        B (numpy.ndarray): [d, d] weighted matrix.\n",
    "        graph_thres (float): Threshold for weighted matrix. Default: 0.3.\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: [d, d] weighted matrix of DAG.\n",
    "    \"\"\"\n",
    "    B = np.copy(B)\n",
    "    B[np.abs(B) <= graph_thres] = 0    # Thresholding\n",
    "    B, _ = utils.threshold_till_dag(B)\n",
    "\n",
    "    return B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Struct Learn via Flex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`flex.struct_learn` is a function that requires the following 5 inputs:\n",
    "- `dataset`: A data matrix with `n x d` shape, where `n` is the number of samples and `d` is the number of variables/features.\n",
    "\n",
    "- `model`: The SEM module. We provide standard SEM implementations such as---LinearModel, LogisticModel, MLP\n",
    "\n",
    "- `constrained_solver`: An instance of a ConstrainedSolver class. We provide implementations such as `PathFollowing` and `AugmentedLagrangian`\n",
    "\n",
    "- `unconstrained_solver`: An instance of an UnconstrainedSolver class. We provide implementation for `GradientBasedSolver`\n",
    "\n",
    "- `loss_fn`: Instance of a Loss class. All available losses can be found at flex/loss.py\n",
    "\n",
    "- `dag_fn`: Instance of a DagFn class. All available DAG functions can be found at flex/dags.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate linear data\n",
    "n, d, s0, graph_type, sem_type = 1000, 20, 20, \"ER\", \"gauss\"\n",
    "linear_B_true = utils.simulate_dag(d, s0, graph_type)\n",
    "linear_dataset = utils.simulate_linear_sem(linear_B_true, n, sem_type)\n",
    "\n",
    "# Generate non-linear data\n",
    "n, d, s0, graph_type, sem_type = 1000, 5, 5, \"ER\", \"mlp\"\n",
    "nonlinear_B_true = utils.simulate_dag(d, s0, graph_type)\n",
    "nonlinear_dataset = utils.simulate_nonlinear_sem(nonlinear_B_true, n, sem_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using flex to implement NOTEARS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## linear NOTEARS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:37<00:00,  3.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Time: 37.8957781791687\n",
      "Results:  {'fdr': 0.0, 'tpr': 1.0, 'fpr': 0.0, 'shd': 0, 'nnz': 20}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Linear model\n",
    "model = flex.LinearModel(d)\n",
    "\n",
    "# Use AML to solve the constrained problem\n",
    "cons_solver = flex.AugmentedLagrangian(\n",
    "    num_iter=10,\n",
    "    num_steps=[3e4,6e4],\n",
    "    l1_coeff=0.03,\n",
    ")\n",
    "\n",
    "# Use Adam to solve the unconstrained problem\n",
    "uncons_solver = flex.GradientBasedSolver(\n",
    "    optimizer=torch.optim.Adam(model.parameters(), lr=3e-4),\n",
    ")\n",
    "\n",
    "# Use MSE loss\n",
    "loss_fn = flex.MSELoss()\n",
    "\n",
    "# Use Trace of matrix exponential as DAG function\n",
    "dag_fn = flex.Exp()\n",
    "\n",
    "# Learn the DAG\n",
    "W_est = flex.struct_learn(\n",
    "    dataset=linear_dataset,\n",
    "    model=model,\n",
    "    constrained_solver=cons_solver,\n",
    "    unconstrained_solver=uncons_solver,\n",
    "    loss_fn=loss_fn,\n",
    "    dag_fn=dag_fn,\n",
    "    w_threshold=0.3,\n",
    ")\n",
    "\n",
    "acc = utils.count_accuracy(linear_B_true, W_est != 0)\n",
    "print(\"Results: \", acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## nonlinear NOTEARS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grandag_aug_lagrangian(n, d, s0, num_layers=2):\n",
    "    graph_type, sem_type = \"ER\", \"mlp\"\n",
    "    nonlinear_B_true = utils.simulate_dag(d, s0, graph_type)\n",
    "    nonlinear_dataset = utils.simulate_nonlinear_sem(nonlinear_B_true, n, sem_type)\n",
    "    print(f'b_true is {nonlinear_B_true}')\n",
    "\n",
    "    train_samples = int(nonlinear_dataset.shape[0] * 0.8)\n",
    "    test_samples = nonlinear_dataset.shape[0] - train_samples\n",
    "    train_dataset = nonlinear_dataset[:train_samples, :]\n",
    "    test_dataset = nonlinear_dataset[train_samples:, :]\n",
    "\n",
    "    # Nonlinear model\n",
    "    model = flex.MLP(dims=[d, 2, d], num_layers=num_layers, hid_dim=10, activation=\"sigmoid\", bias=True)\n",
    "\n",
    "    # Use AML to solve the constrained problem\n",
    "    cons_solver = flex.AugmentedLagrangian(\n",
    "        num_iter=100000,\n",
    "        num_steps=[1,1],\n",
    "        # l1_coeff=0.01,\n",
    "        # weight_decay=0.01,\n",
    "        rho_init=1e-3,\n",
    "    )\n",
    "\n",
    "    # Use Adam to solve the unconstrained problem\n",
    "    uncons_solver = flex.GradientBasedSolver(\n",
    "        optimizer=torch.optim.RMSprop(model.parameters(), lr=1e-3),\n",
    "    )\n",
    "\n",
    "    # Use MSE loss\n",
    "    loss_fn = flex.MSELoss()\n",
    "\n",
    "    # Use Trace of matrix exponential as DAG function\n",
    "    dag_fn = flex.Grandag_h()\n",
    "\n",
    "    # Learn the DAG\n",
    "    W_est = flex.struct_learn(\n",
    "        dataset=train_dataset,\n",
    "        model=model,\n",
    "        constrained_solver=cons_solver,\n",
    "        unconstrained_solver=uncons_solver,\n",
    "        loss_fn=loss_fn,\n",
    "        dag_fn=dag_fn,\n",
    "        w_threshold=0.0,\n",
    "    )\n",
    "\n",
    "    W_est = postprocess(W_est)\n",
    "\n",
    "    # opt = {\n",
    "    #     cam_pruning_cutoff: np.logspace(-6, 0, 10),\n",
    "    #     exp_path: 'cam-pruning',\n",
    "    # }\n",
    "    # try:\n",
    "    #     cam_pruning_cutoff = [float(i) for i in opt.cam_pruning_cutoff]\n",
    "    # except:\n",
    "    #     cam_pruning_cutoff = [float(opt.cam_pruning_cutoff)]\n",
    "    # W_est =  cam_pruning(model, train_data, test_data, opt, cutoff=cutoff,\n",
    "    #                             metrics_callback=metrics_callback, plotting_callback=plotting_callback)\n",
    "    acc = utils.count_accuracy(nonlinear_B_true, W_est != 0)\n",
    "    print(\"Results: \", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b_true is [[0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100000/100000 [00:20<00:00, 4914.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h new is 0.0 and h tol is 1e-08 so ending at 9973th iteration\n",
      "Total Time: 20.349923133850098\n",
      "Results:  {'fdr': 0.7, 'tpr': 0.6, 'fpr': 1.4, 'shd': 7, 'nnz': 10}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "grandag_aug_lagrangian(1000, 5, 5, num_layers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/seren/dagrad/dagrad/flex/modules/constrained_solvers.py:164: UserWarning: Using the first value from num_steps for the first 99998 iterations\n",
      "  else:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b_true is [[0. 1. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [1. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100000/100000 [00:34<00:00, 2926.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h new is 0.0 and h tol is 1e-08 so ending at 15256th iteration\n",
      "Total Time: 34.181851863861084\n",
      "Results:  {'fdr': 0.5, 'tpr': 0.8, 'fpr': 0.8, 'shd': 5, 'nnz': 8}\n",
      "b_true is [[0. 0. 1. 1. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 1. 1. 0. 0.]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100000/100000 [00:22<00:00, 4388.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h new is 0.0 and h tol is 1e-08 so ending at 11406th iteration\n",
      "Total Time: 22.849276065826416\n",
      "Results:  {'fdr': 0.625, 'tpr': 0.6, 'fpr': 1.0, 'shd': 5, 'nnz': 8}\n",
      "b_true is [[0. 0. 0. 0. 0.]\n",
      " [1. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100000/100000 [00:37<00:00, 2679.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h new is 0.0 and h tol is 1e-08 so ending at 18362th iteration\n",
      "Total Time: 37.32694911956787\n",
      "Results:  {'fdr': 0.3333333333333333, 'tpr': 0.8, 'fpr': 0.4, 'shd': 3, 'nnz': 6}\n",
      "b_true is [[0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 1. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100000/100000 [00:27<00:00, 3590.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h new is 0.0 and h tol is 1e-08 so ending at 14066th iteration\n",
      "Total Time: 27.89175295829773\n",
      "Results:  {'fdr': 0.5555555555555556, 'tpr': 0.8, 'fpr': 1.0, 'shd': 5, 'nnz': 9}\n",
      "b_true is [[0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 1.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [1. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100000/100000 [00:24<00:00, 4000.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h new is 0.0 and h tol is 1e-08 so ending at 12607th iteration\n",
      "Total Time: 25.043652057647705\n",
      "Results:  {'fdr': 0.6, 'tpr': 0.8, 'fpr': 1.2, 'shd': 6, 'nnz': 10}\n",
      "b_true is [[0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 1. 1.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100000/100000 [00:32<00:00, 3036.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h new is 0.0 and h tol is 1e-08 so ending at 16743th iteration\n",
      "Total Time: 32.94796299934387\n",
      "Results:  {'fdr': 0.75, 'tpr': 0.4, 'fpr': 1.2, 'shd': 7, 'nnz': 8}\n",
      "b_true is [[0. 0. 0. 1. 1.]\n",
      " [1. 0. 0. 1. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100000/100000 [00:34<00:00, 2910.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h new is 0.0 and h tol is 1e-08 so ending at 17611th iteration\n",
      "Total Time: 34.37272906303406\n",
      "Results:  {'fdr': 0.5555555555555556, 'tpr': 0.8, 'fpr': 1.0, 'shd': 6, 'nnz': 9}\n",
      "b_true is [[0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 1. 0.]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100000/100000 [00:24<00:00, 4140.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h new is 0.0 and h tol is 1e-08 so ending at 12036th iteration\n",
      "Total Time: 24.235503911972046\n",
      "Results:  {'fdr': 0.42857142857142855, 'tpr': 0.8, 'fpr': 0.6, 'shd': 3, 'nnz': 7}\n",
      "b_true is [[0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 1. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [1. 0. 1. 0. 0.]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100000/100000 [00:25<00:00, 3882.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h new is 0.0 and h tol is 1e-08 so ending at 13207th iteration\n",
      "Total Time: 25.86980891227722\n",
      "Results:  {'fdr': 0.4444444444444444, 'tpr': 1.0, 'fpr': 0.8, 'shd': 4, 'nnz': 9}\n",
      "b_true is [[0. 0. 0. 0. 0.]\n",
      " [1. 0. 1. 1. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100000/100000 [00:39<00:00, 2509.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h new is 0.0 and h tol is 1e-08 so ending at 20405th iteration\n",
      "Total Time: 39.864755153656006\n",
      "Results:  {'fdr': 0.5, 'tpr': 0.6, 'fpr': 0.6, 'shd': 4, 'nnz': 6}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    grandag_aug_lagrangian(1000, 5, 5, num_layers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/seren/dagrad/dagrad/flex/modules/constrained_solvers.py:160: UserWarning: Using the first value from num_steps for the first 99998 iterations\n",
      "  self.vwarn(f\"Using the first value from num_steps for the first {missing_steps} iterations\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b_true is [[0. 0. 0. 0. 1. 1. 1. 0. 0. 0.]\n",
      " [0. 0. 1. 1. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100000/100000 [03:55<00:00, 424.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h new is 0.0 and h tol is 1e-08 so ending at 10409th iteration\n",
      "Total Time: 235.45690202713013\n",
      "Results:  {'fdr': 0.7631578947368421, 'tpr': 0.9, 'fpr': 0.8285714285714286, 'shd': 29, 'nnz': 38}\n"
     ]
    }
   ],
   "source": [
    "grandag_aug_lagrangian(1000, 10, 10, num_layers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/seren/dagrad/dagrad/flex/modules/constrained_solvers.py:160: UserWarning: Using the first value from num_steps for the first 99998 iterations\n",
      "  self.vwarn(f\"Using the first value from num_steps for the first {missing_steps} iterations\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b_true is [[0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100000/100000 [07:06<00:00, 234.64it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h new is 0.0 and h tol is 1e-08 so ending at 13021th iteration\n",
      "Total Time: 426.18568086624146\n",
      "Results:  {'fdr': 0.8689655172413793, 'tpr': 0.95, 'fpr': 0.7411764705882353, 'shd': 126, 'nnz': 145}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "grandag_aug_lagrangian(1000, 20, 20, num_layers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using flex to implement DAGMA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear DAGMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/seren/dagrad/dagrad/flex/modules/constrained_solvers.py:49: UserWarning: Using the first value from num_steps for the first 3 iterations\n",
      "  self.vwarn(\n",
      "100%|██████████| 5/5 [00:08<00:00,  1.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Time: 8.867684841156006\n",
      "Results:  {'fdr': 0.0, 'tpr': 1.0, 'fpr': 0.0, 'shd': 0, 'nnz': 20}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Linear model\n",
    "model = flex.LinearModel(d)\n",
    "\n",
    "# Use path following to solve the constrained problem\n",
    "cons_solver = flex.PathFollowing(\n",
    "    num_iter=5,\n",
    "    mu_init=1.0,\n",
    "    mu_scale=0.1,\n",
    "    logdet_coeff=[1.0, .9, .8, .7, .6],\n",
    "    num_steps=[3e4, 6e4],\n",
    "    l1_coeff=0.03,\n",
    ")\n",
    "\n",
    "# use Adam to solve the unconstrained problem\n",
    "uncons_solver = flex.GradientBasedSolver(\n",
    "    optimizer=torch.optim.Adam(model.parameters(), lr=3e-4, betas=(0.99, 0.999))\n",
    ")\n",
    "\n",
    "# Use MSE loss\n",
    "loss_fn = flex.MSELoss()\n",
    "\n",
    "# Use LogDet as DAG function\n",
    "dag_fn = flex.LogDet()\n",
    "\n",
    "W_est = flex.struct_learn(\n",
    "    dataset=linear_dataset,\n",
    "    model=model,\n",
    "    constrained_solver=cons_solver,\n",
    "    unconstrained_solver=uncons_solver,\n",
    "    loss_fn=loss_fn,\n",
    "    dag_fn=dag_fn,\n",
    "    w_threshold=0.3,\n",
    ")\n",
    "\n",
    "acc = utils.count_accuracy(linear_B_true, W_est != 0)\n",
    "print(\"Results: \", acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nonlinear DAGMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/seren/dagrad/dagrad/flex/modules/constrained_solvers.py:51: UserWarning: Using the first value from num_steps for the first 2 iterations\n",
      "  self.vwarn(\n",
      "100%|██████████| 4/4 [09:40<00:00, 145.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Time: 580.489175081253\n",
      "Results:  {'fdr': 0.0, 'tpr': 0.0, 'fpr': 0.0, 'shd': 5, 'nnz': 0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "n, d, s0, graph_type, sem_type = 1000, 5, 5, \"ER\", \"mlp\"\n",
    "nonlinear_B_true = utils.simulate_dag(d, s0, graph_type)\n",
    "nonlinear_dataset = utils.simulate_nonlinear_sem(nonlinear_B_true, n, sem_type)\n",
    "\n",
    "# nonlinear model\n",
    "model = flex.MLP(dims=[d, 2, 5], num_layers=2, hid_dim=10, activation=\"sigmoid\", bias=True)\n",
    "\n",
    "# Use path following to solve the constrained problem\n",
    "cons_solver = flex.PathFollowing(\n",
    "    num_iter=4,\n",
    "    mu_init=0.1,\n",
    "    mu_scale=0.1,\n",
    "    logdet_coeff=1.0,\n",
    "    num_steps=[5e4, 8e4],\n",
    "    weight_decay=0.02,\n",
    "    l1_coeff=0.005,\n",
    ")\n",
    "\n",
    "# use Adam to solve the unconstrained problem\n",
    "uncons_solver = flex.GradientBasedSolver(\n",
    "    # optimizer=torch.optim.Adam(model.parameters(), lr=2e-4, betas=(0.99, 0.999))\n",
    "    optimizer=torch.optim.SGD(model.parameters(), lr=2e-4)\n",
    ")\n",
    "\n",
    "# Use NLL loss\n",
    "loss_fn = flex.NLLLoss()\n",
    "\n",
    "# Use LogDet as DAG function\n",
    "dag_fn = flex.LogDet()\n",
    "\n",
    "# Learn the DAG\n",
    "W_est = flex.struct_learn(\n",
    "    dataset=nonlinear_dataset,\n",
    "    model=model,\n",
    "    constrained_solver=cons_solver,\n",
    "    unconstrained_solver=uncons_solver,\n",
    "    loss_fn=loss_fn,\n",
    "    dag_fn=dag_fn,\n",
    "    w_threshold=0.3,\n",
    ")\n",
    "\n",
    "W_est = postprocess(W_est)\n",
    "acc = utils.count_accuracy(nonlinear_B_true, W_est != 0)\n",
    "print(\"Results: \", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
