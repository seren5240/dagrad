{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x10b7adb70>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "import torch\n",
    "import dagrad.flex as flex\n",
    "from dagrad.utils import utils\n",
    "\n",
    "utils.set_random_seed(1)\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Struct Learn via Flex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`flex.struct_learn` is a function that requires the following 5 inputs:\n",
    "- `dataset`: A data matrix with `n x d` shape, where `n` is the number of samples and `d` is the number of variables/features.\n",
    "\n",
    "- `model`: The SEM module. We provide standard SEM implementations such as---LinearModel, LogisticModel, MLP\n",
    "\n",
    "- `constrained_solver`: An instance of a ConstrainedSolver class. We provide implementations such as `PathFollowing` and `AugmentedLagrangian`\n",
    "\n",
    "- `unconstrained_solver`: An instance of an UnconstrainedSolver class. We provide implementation for `GradientBasedSolver`\n",
    "\n",
    "- `loss_fn`: Instance of a Loss class. All available losses can be found at flex/loss.py\n",
    "\n",
    "- `dag_fn`: Instance of a DagFn class. All available DAG functions can be found at flex/dags.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate linear data\n",
    "n, d, s0, graph_type, sem_type = 1000, 20, 20, \"ER\", \"gauss\"\n",
    "linear_B_true = utils.simulate_dag(d, s0, graph_type)\n",
    "linear_dataset = utils.simulate_linear_sem(linear_B_true, n, sem_type)\n",
    "\n",
    "# Generate non-linear data\n",
    "n, d, s0, graph_type, sem_type = 1000, 20, 20, \"ER\", \"mlp\"\n",
    "nonlinear_B_true = utils.simulate_dag(d, s0, graph_type)\n",
    "nonlinear_dataset = utils.simulate_nonlinear_sem(nonlinear_B_true, n, sem_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using flex to implement NOTEARS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## linear NOTEARS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:37<00:00,  3.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Time: 37.8957781791687\n",
      "Results:  {'fdr': 0.0, 'tpr': 1.0, 'fpr': 0.0, 'shd': 0, 'nnz': 20}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Linear model\n",
    "model = flex.LinearModel(d)\n",
    "\n",
    "# Use AML to solve the constrained problem\n",
    "cons_solver = flex.AugmentedLagrangian(\n",
    "    num_iter=10,\n",
    "    num_steps=[3e4,6e4],\n",
    "    l1_coeff=0.03,\n",
    ")\n",
    "\n",
    "# Use Adam to solve the unconstrained problem\n",
    "uncons_solver = flex.GradientBasedSolver(\n",
    "    optimizer=torch.optim.Adam(model.parameters(), lr=3e-4),\n",
    ")\n",
    "\n",
    "# Use MSE loss\n",
    "loss_fn = flex.MSELoss()\n",
    "\n",
    "# Use Trace of matrix exponential as DAG function\n",
    "dag_fn = flex.Exp()\n",
    "\n",
    "# Learn the DAG\n",
    "W_est = flex.struct_learn(\n",
    "    dataset=linear_dataset,\n",
    "    model=model,\n",
    "    constrained_solver=cons_solver,\n",
    "    unconstrained_solver=uncons_solver,\n",
    "    loss_fn=loss_fn,\n",
    "    dag_fn=dag_fn,\n",
    "    w_threshold=0.3,\n",
    ")\n",
    "\n",
    "acc = utils.count_accuracy(linear_B_true, W_est != 0)\n",
    "print(\"Results: \", acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## nonlinear NOTEARS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/seren/dagrad/dagrad/flex/modules/constrained_solvers.py:159: UserWarning: Using the first value from num_steps for the first 8 iterations\n",
      "  self.vwarn(f\"Using the first value from num_steps for the first {missing_steps} iterations\")\n",
      "100%|██████████| 10/10 [05:13<00:00, 31.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Time: 313.17299032211304\n",
      "Results:  {'fdr': 1.0, 'tpr': 0.0, 'fpr': 0.023529411764705882, 'shd': 24, 'nnz': 4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Nonlinear model\n",
    "model = flex.MLP(dims=[d, 10, 1], activation=\"sigmoid\", bias=True)\n",
    "\n",
    "# Use AML to solve the constrained problem\n",
    "cons_solver = flex.AugmentedLagrangian(\n",
    "    num_iter=10,\n",
    "    num_steps=[4e4,6e4],\n",
    "    l1_coeff=0.01,\n",
    "    weight_decay=0.01,\n",
    ")\n",
    "\n",
    "# Use Adam to solve the unconstrained problem\n",
    "uncons_solver = flex.GradientBasedSolver(\n",
    "    optimizer=torch.optim.Adam(model.parameters(), lr=3e-4),\n",
    ")\n",
    "\n",
    "# Use MSE loss\n",
    "loss_fn = flex.MSELoss()\n",
    "\n",
    "# Use Trace of matrix exponential as DAG function\n",
    "dag_fn = flex.Exp()\n",
    "\n",
    "# Learn the DAG\n",
    "W_est = flex.struct_learn(\n",
    "    dataset=nonlinear_dataset,\n",
    "    model=model,\n",
    "    constrained_solver=cons_solver,\n",
    "    unconstrained_solver=uncons_solver,\n",
    "    loss_fn=loss_fn,\n",
    "    dag_fn=dag_fn,\n",
    "    w_threshold=0.3,\n",
    ")\n",
    "\n",
    "acc = utils.count_accuracy(nonlinear_B_true, W_est != 0)\n",
    "print(\"Results: \", acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using flex to implement DAGMA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear DAGMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/seren/dagrad/dagrad/flex/modules/constrained_solvers.py:49: UserWarning: Using the first value from num_steps for the first 3 iterations\n",
      "  self.vwarn(\n",
      "100%|██████████| 5/5 [00:08<00:00,  1.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Time: 8.867684841156006\n",
      "Results:  {'fdr': 0.0, 'tpr': 1.0, 'fpr': 0.0, 'shd': 0, 'nnz': 20}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Linear model\n",
    "model = flex.LinearModel(d)\n",
    "\n",
    "# Use path following to solve the constrained problem\n",
    "cons_solver = flex.PathFollowing(\n",
    "    num_iter=5,\n",
    "    mu_init=1.0,\n",
    "    mu_scale=0.1,\n",
    "    logdet_coeff=[1.0, .9, .8, .7, .6],\n",
    "    num_steps=[3e4, 6e4],\n",
    "    l1_coeff=0.03,\n",
    ")\n",
    "\n",
    "# use Adam to solve the unconstrained problem\n",
    "uncons_solver = flex.GradientBasedSolver(\n",
    "    optimizer=torch.optim.Adam(model.parameters(), lr=3e-4, betas=(0.99, 0.999))\n",
    ")\n",
    "\n",
    "# Use MSE loss\n",
    "loss_fn = flex.MSELoss()\n",
    "\n",
    "# Use LogDet as DAG function\n",
    "dag_fn = flex.LogDet()\n",
    "\n",
    "W_est = flex.struct_learn(\n",
    "    dataset=linear_dataset,\n",
    "    model=model,\n",
    "    constrained_solver=cons_solver,\n",
    "    unconstrained_solver=uncons_solver,\n",
    "    loss_fn=loss_fn,\n",
    "    dag_fn=dag_fn,\n",
    "    w_threshold=0.3,\n",
    ")\n",
    "\n",
    "acc = utils.count_accuracy(linear_B_true, W_est != 0)\n",
    "print(\"Results: \", acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nonlinear DAGMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/seren/dagrad/dagrad/flex/modules/constrained_solvers.py:51: UserWarning: Using the first value from num_steps for the first 2 iterations\n",
      "  self.vwarn(\n",
      "  0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total loss: 7.746262377447302\n",
      "total loss: 7.745656129937287\n",
      "total loss: 7.745049946564558\n",
      "total loss: 7.744443827463069\n",
      "total loss: 7.743837772766753\n",
      "total loss: 7.7432317826095325\n",
      "total loss: 7.742625857125301\n",
      "total loss: 7.742019996447937\n",
      "total loss: 7.741414200711286\n",
      "total loss: 7.74080847004916\n",
      "total loss: 7.740202804595346\n",
      "total loss: 7.739597204483589\n",
      "total loss: 7.738991669847595\n",
      "total loss: 7.738386200821026\n",
      "total loss: 7.737780797537501\n",
      "total loss: 7.737175460130586\n",
      "total loss: 7.736570188733799\n",
      "total loss: 7.735964983480598\n",
      "total loss: 7.7353598445043845\n",
      "total loss: 7.734754771938498\n",
      "total loss: 7.734149765916213\n",
      "total loss: 7.733544826570736\n",
      "total loss: 7.732939954035201\n",
      "total loss: 7.732335148442671\n",
      "total loss: 7.731730409926125\n",
      "total loss: 7.731125738618471\n",
      "total loss: 7.730521134652523\n",
      "total loss: 7.729916598161013\n",
      "total loss: 7.729312129276586\n",
      "total loss: 7.728707728131785\n",
      "total loss: 7.728103394859066\n",
      "total loss: 7.727499129590781\n",
      "total loss: 7.726894932459177\n",
      "total loss: 7.726290803596401\n",
      "total loss: 7.725686743134487\n",
      "total loss: 7.725082751205358\n",
      "total loss: 7.724478827940824\n",
      "total loss: 7.723874973472571\n",
      "total loss: 7.723271187932175\n",
      "total loss: 7.722667471451073\n",
      "total loss: 7.722063824160589\n",
      "total loss: 7.7214602461919055\n",
      "total loss: 7.720856737676076\n",
      "total loss: 7.720253298744018\n",
      "total loss: 7.719649929526507\n",
      "total loss: 7.719046630154177\n",
      "total loss: 7.718443400757521\n",
      "total loss: 7.71784024146687\n",
      "total loss: 7.71723715241242\n",
      "total loss: 7.716634133724199\n",
      "total loss: 7.716031185532085\n",
      "total loss: 7.715428307965791\n",
      "total loss: 7.714825501154865\n",
      "total loss: 7.714222765228693\n",
      "total loss: 7.71362010031649\n",
      "total loss: 7.713017506547293\n",
      "total loss: 7.712414984049969\n",
      "total loss: 7.711812532953208\n",
      "total loss: 7.711210153385511\n",
      "total loss: 7.710607845475199\n",
      "total loss: 7.7100056093504055\n",
      "total loss: 7.709403445139073\n",
      "total loss: 7.708801352968952\n",
      "total loss: 7.708199332967595\n",
      "total loss: 7.707597385262358\n",
      "total loss: 7.706995509980393\n",
      "total loss: 7.7063937072486475\n",
      "total loss: 7.705791977193866\n",
      "total loss: 7.705190319942576\n",
      "total loss: 7.704588735621097\n",
      "total loss: 7.703987224355529\n",
      "total loss: 7.703385786271759\n",
      "total loss: 7.702784421495449\n",
      "total loss: 7.702183130152036\n",
      "total loss: 7.701581912366733\n",
      "total loss: 7.7009807682645235\n",
      "total loss: 7.700379697970155\n",
      "total loss: 7.69977870160815\n",
      "total loss: 7.699177779302781\n",
      "total loss: 7.698576931178092\n",
      "total loss: 7.697976157357878\n",
      "total loss: 7.697375457965693\n",
      "total loss: 7.696774833124841\n",
      "total loss: 7.6961742829583715\n",
      "total loss: 7.695573807589093\n",
      "total loss: 7.694973407139548\n",
      "total loss: 7.694373081732029\n",
      "total loss: 7.693772831488563\n",
      "total loss: 7.693172656530919\n",
      "total loss: 7.692572556980601\n",
      "total loss: 7.691972532958844\n",
      "total loss: 7.691372584586612\n",
      "total loss: 7.690772711984602\n",
      "total loss: 7.690172915273235\n",
      "total loss: 7.689573194572652\n",
      "total loss: 7.688973550002725\n",
      "total loss: 7.688373981683035\n",
      "total loss: 7.687774489732884\n",
      "total loss: 7.687175074271296\n",
      "total loss: 7.6865757354169935\n",
      "total loss: 7.685976473288425\n",
      "total loss: 7.685377288003737\n",
      "total loss: 7.684778179680789\n",
      "total loss: 7.684179148437141\n",
      "total loss: 7.683580194390061\n",
      "total loss: 7.682981317656509\n",
      "total loss: 7.682382518353156\n",
      "total loss: 7.681783796596362\n",
      "total loss: 7.681185152502182\n",
      "total loss: 7.68058658618637\n",
      "total loss: 7.679988097764365\n",
      "total loss: 7.679389687351306\n",
      "total loss: 7.678791355062009\n",
      "total loss: 7.678193101010981\n",
      "total loss: 7.677594925312417\n",
      "total loss: 7.676996828080195\n",
      "total loss: 7.6763988094278695\n",
      "total loss: 7.675800869468683\n",
      "total loss: 7.6752030083155525\n",
      "total loss: 7.674605226081071\n",
      "total loss: 7.674007522877512\n",
      "total loss: 7.673409898816825\n",
      "total loss: 7.672812354010626\n",
      "total loss: 7.672214888570209\n",
      "total loss: 7.671617502606538\n",
      "total loss: 7.671020196230246\n",
      "total loss: 7.670422969551635\n",
      "total loss: 7.669825822680676\n",
      "total loss: 7.669228755727006\n",
      "total loss: 7.6686317687999255\n",
      "total loss: 7.6680348620084\n",
      "total loss: 7.667438035461067\n",
      "total loss: 7.666841289266211\n",
      "total loss: 7.666244623531792\n",
      "total loss: 7.665648038365427\n",
      "total loss: 7.6650515338743945\n",
      "total loss: 7.664455110165628\n",
      "total loss: 7.663858767345725\n",
      "total loss: 7.663262505520941\n",
      "total loss: 7.662666324797188\n",
      "total loss: 7.662070225280035\n",
      "total loss: 7.66147420707471\n",
      "total loss: 7.660878270286098\n",
      "total loss: 7.660282415018735\n",
      "total loss: 7.659686641376822\n",
      "total loss: 7.659090949464204\n",
      "total loss: 7.65849533938439\n",
      "total loss: 7.6578998112405445\n",
      "total loss: 7.657304365135481\n",
      "total loss: 7.656709001171669\n",
      "total loss: 7.656113719451237\n",
      "total loss: 7.655518520075964\n",
      "total loss: 7.654923403147289\n",
      "total loss: 7.654328368766301\n",
      "total loss: 7.653733417033742\n",
      "total loss: 7.653138548050017\n",
      "total loss: 7.6525437619151795\n",
      "total loss: 7.651949058728942\n",
      "total loss: 7.65135443859067\n",
      "total loss: 7.650759901599387\n",
      "total loss: 7.650165447853777\n",
      "total loss: 7.649571077452171\n",
      "total loss: 7.6489767904925685\n",
      "total loss: 7.648382587072619\n",
      "total loss: 7.647788467289634\n",
      "total loss: 7.647194431240584\n",
      "total loss: 7.646600479022096\n",
      "total loss: 7.646006610730463\n",
      "total loss: 7.645412826461635\n",
      "total loss: 7.644819126311221\n",
      "total loss: 7.644225510374499\n",
      "total loss: 7.643631978746405\n",
      "total loss: 7.643038531521543\n",
      "total loss: 7.642445168794176\n",
      "total loss: 7.64185189065824\n",
      "total loss: 7.6412586972073315\n",
      "total loss: 7.640665588534719\n",
      "total loss: 7.6400725647333365\n",
      "total loss: 7.639479625895788\n",
      "total loss: 7.638886772114349\n",
      "total loss: 7.638294003480967\n",
      "total loss: 7.637701320087258\n",
      "total loss: 7.637108722024519\n",
      "total loss: 7.636516209383713\n",
      "total loss: 7.635923782255489\n",
      "total loss: 7.635331440730163\n",
      "total loss: 7.634739184897735\n",
      "total loss: 7.634147014847884\n",
      "total loss: 7.633554930669965\n",
      "total loss: 7.632962932453022\n",
      "total loss: 7.6323710202857775\n",
      "total loss: 7.631779194256635\n",
      "total loss: 7.6311874544536895\n",
      "total loss: 7.630595800964717\n",
      "total loss: 7.630004233877189\n",
      "total loss: 7.629412753278259\n",
      "total loss: 7.628821359254772\n",
      "total loss: 7.628230051893263\n",
      "total loss: 7.627638831279967\n",
      "total loss: 7.627047697500801\n",
      "total loss: 7.6264566506413916\n",
      "total loss: 7.625865690787046\n",
      "total loss: 7.625274818022785\n",
      "total loss: 7.624684032433314\n",
      "total loss: 7.624093334103047\n",
      "total loss: 7.623502723116096\n",
      "total loss: 7.622912199556275\n",
      "total loss: 7.622321763507103\n",
      "total loss: 7.621731415051807\n",
      "total loss: 7.621141154273309\n",
      "total loss: 7.62055098125425\n",
      "total loss: 7.619960896076973\n",
      "total loss: 7.61937089882353\n",
      "total loss: 7.618780989575685\n",
      "total loss: 7.618191168414912\n",
      "total loss: 7.617601435422396\n",
      "total loss: 7.617011790679038\n",
      "total loss: 7.616422234265457\n",
      "total loss: 7.615832766261973\n",
      "total loss: 7.61524338674864\n",
      "total loss: 7.614654095805215\n",
      "total loss: 7.614064893511182\n",
      "total loss: 7.613475779945738\n",
      "total loss: 7.612886755187803\n",
      "total loss: 7.612297819316015\n",
      "total loss: 7.611708972408733\n",
      "total loss: 7.611120214544044\n",
      "total loss: 7.610531545799749\n",
      "total loss: 7.609942966253381\n",
      "total loss: 7.609354475982185\n",
      "total loss: 7.608766075063142\n",
      "total loss: 7.608177763572952\n",
      "total loss: 7.6075895415880455\n",
      "total loss: 7.607001409184574\n",
      "total loss: 7.606413366438418\n",
      "total loss: 7.605825413425185\n",
      "total loss: 7.605237550220212\n",
      "total loss: 7.604649776898559\n",
      "total loss: 7.604062093535022\n",
      "total loss: 7.603474500204117\n",
      "total loss: 7.602886996980095\n",
      "total loss: 7.6022995839369365\n",
      "total loss: 7.601712261148344\n",
      "total loss: 7.601125028687762\n",
      "total loss: 7.600537886628353\n",
      "total loss: 7.599950835043018\n",
      "total loss: 7.599363874004383\n",
      "total loss: 7.5987770035848055\n",
      "total loss: 7.598190223856375\n",
      "total loss: 7.597603534890909\n",
      "total loss: 7.597016936759957\n",
      "total loss: 7.596430429534794\n",
      "total loss: 7.595844013286437\n",
      "total loss: 7.595257688085622\n",
      "total loss: 7.5946714540028175\n",
      "total loss: 7.594085311108223\n",
      "total loss: 7.593499259471766\n",
      "total loss: 7.592913299163108\n",
      "total loss: 7.59232743025164\n",
      "total loss: 7.59174165280647\n",
      "total loss: 7.591155966896451\n",
      "total loss: 7.590570372590153\n",
      "total loss: 7.589984869955884\n",
      "total loss: 7.589399459061668\n",
      "total loss: 7.588814139975272\n",
      "total loss: 7.588228912764175\n",
      "total loss: 7.587643777495593\n",
      "total loss: 7.587058734236463\n",
      "total loss: 7.586473783053453\n",
      "total loss: 7.585888924012953\n",
      "total loss: 7.585304157181079\n",
      "total loss: 7.584719482623672\n",
      "total loss: 7.584134900406303\n",
      "total loss: 7.583550410594254\n",
      "total loss: 7.582966013252546\n",
      "total loss: 7.58238170844591\n",
      "total loss: 7.581797496238811\n",
      "total loss: 7.581213376695424\n",
      "total loss: 7.580629349879656\n",
      "total loss: 7.580045415855132\n",
      "total loss: 7.579461574685192\n",
      "total loss: 7.578877826432904\n",
      "total loss: 7.578294171161053\n",
      "total loss: 7.57771060893214\n",
      "total loss: 7.577127139808389\n",
      "total loss: 7.576543763851738\n",
      "total loss: 7.575960481123844\n",
      "total loss: 7.575377291686085\n",
      "total loss: 7.574794195599547\n",
      "total loss: 7.574211192925039\n",
      "total loss: 7.573628283723083\n",
      "total loss: 7.5730454680539125\n",
      "total loss: 7.572462745977484\n",
      "total loss: 7.571880117553458\n",
      "total loss: 7.571297582841217\n",
      "total loss: 7.570715141899849\n",
      "total loss: 7.570132794788158\n",
      "total loss: 7.569550541564659\n",
      "total loss: 7.568968382287583\n",
      "total loss: 7.568386317014864\n",
      "total loss: 7.56780434580415\n",
      "total loss: 7.567222468712802\n",
      "total loss: 7.566640685797885\n",
      "total loss: 7.56605899711618\n",
      "total loss: 7.565477402724167\n",
      "total loss: 7.564895902678048\n",
      "total loss: 7.564314497033718\n",
      "total loss: 7.56373318584679\n",
      "total loss: 7.563151969172582\n",
      "total loss: 7.562570847066111\n",
      "total loss: 7.561989819582116\n",
      "total loss: 7.561408886775025\n",
      "total loss: 7.560828048698988\n",
      "total loss: 7.560247305407845\n",
      "total loss: 7.559666656955154\n",
      "total loss: 7.559086103394168\n",
      "total loss: 7.558505644777851\n",
      "total loss: 7.557925281158871\n",
      "total loss: 7.557345012589593\n",
      "total loss: 7.556764839122098\n",
      "total loss: 7.556184760808159\n",
      "total loss: 7.555604777699258\n",
      "total loss: 7.555024889846581\n",
      "total loss: 7.5544450973010155\n",
      "total loss: 7.553865400113152\n",
      "total loss: 7.553285798333283\n",
      "total loss: 7.552706292011406\n",
      "total loss: 7.552126881197214\n",
      "total loss: 7.5515475659401154\n",
      "total loss: 7.550968346289211\n",
      "total loss: 7.550389222293304\n",
      "total loss: 7.549810194000904\n",
      "total loss: 7.549231261460221\n",
      "total loss: 7.548652424719167\n",
      "total loss: 7.5480736838253595\n",
      "total loss: 7.54749503882611\n",
      "total loss: 7.546916489768441\n",
      "total loss: 7.546338036699072\n",
      "total loss: 7.545759679664427\n",
      "total loss: 7.545181418710633\n",
      "total loss: 7.544603253883514\n",
      "total loss: 7.544025185228604\n",
      "total loss: 7.54344721279114\n",
      "total loss: 7.542869336616055\n",
      "total loss: 7.542291556747984\n",
      "total loss: 7.541713873231277\n",
      "total loss: 7.5411362861099756\n",
      "total loss: 7.540558795427832\n",
      "total loss: 7.539981401228297\n",
      "total loss: 7.539404103554526\n",
      "total loss: 7.538826902449384\n",
      "total loss: 7.538249797955435\n",
      "total loss: 7.537672790114948\n",
      "total loss: 7.5370958789698985\n",
      "total loss: 7.536519064561968\n",
      "total loss: 7.535942346932541\n",
      "total loss: 7.53536572612271\n",
      "total loss: 7.534789202173271\n",
      "total loss: 7.53421277512473\n",
      "total loss: 7.533636445017297\n",
      "total loss: 7.53306021189089\n",
      "total loss: 7.532484075785134\n",
      "total loss: 7.531908036739365\n",
      "total loss: 7.5313320947926226\n",
      "total loss: 7.530756249983658\n",
      "total loss: 7.530180502350933\n",
      "total loss: 7.529604851932616\n",
      "total loss: 7.5290292987665834\n",
      "total loss: 7.528453842890428\n",
      "total loss: 7.527878484341453\n",
      "total loss: 7.527303223156667\n",
      "total loss: 7.526728059372795\n",
      "total loss: 7.526152993026275\n",
      "total loss: 7.5255780241532575\n",
      "total loss: 7.525003152789602\n",
      "total loss: 7.524428378970888\n",
      "total loss: 7.5238537027324055\n",
      "total loss: 7.523279124109164\n",
      "total loss: 7.522704643135883\n",
      "total loss: 7.522130259847001\n",
      "total loss: 7.5215559742766755\n",
      "total loss: 7.520981786458776\n",
      "total loss: 7.520407696426893\n",
      "total loss: 7.519833704214337\n",
      "total loss: 7.519259809854135\n",
      "total loss: 7.518686013379036\n",
      "total loss: 7.518112314821506\n",
      "total loss: 7.517538714213736\n",
      "total loss: 7.516965211587635\n",
      "total loss: 7.516391806974835\n",
      "total loss: 7.515818500406695\n",
      "total loss: 7.515245291914295\n",
      "total loss: 7.514672181528434\n",
      "total loss: 7.514099169279646\n",
      "total loss: 7.513526255198181\n",
      "total loss: 7.512953439314023\n",
      "total loss: 7.512380721656879\n",
      "total loss: 7.511808102256183\n",
      "total loss: 7.511235581141101\n",
      "total loss: 7.510663158340524\n",
      "total loss: 7.510090833883078\n",
      "total loss: 7.509518607797113\n",
      "total loss: 7.5089464801107155\n",
      "total loss: 7.508374450851705\n",
      "total loss: 7.507802520047629\n",
      "total loss: 7.5072306877257695\n",
      "total loss: 7.50665895391315\n",
      "total loss: 7.506087318636516\n",
      "total loss: 7.505515781922363\n",
      "total loss: 7.504944343796915\n",
      "total loss: 7.504373004286131\n",
      "total loss: 7.503801763415719\n",
      "total loss: 7.503230621211116\n",
      "total loss: 7.502659577697501\n",
      "total loss: 7.502088632899798\n",
      "total loss: 7.501517786842666\n",
      "total loss: 7.500947039550514\n",
      "total loss: 7.500376391047487\n",
      "total loss: 7.499805841357476\n",
      "total loss: 7.499235390504121\n",
      "total loss: 7.498665038510799\n",
      "total loss: 7.498094785400643\n",
      "total loss: 7.497524631196524\n",
      "total loss: 7.496954575921069\n",
      "total loss: 7.49638461959665\n",
      "total loss: 7.49581476224539\n",
      "total loss: 7.495245003889161\n",
      "total loss: 7.494675344549588\n",
      "total loss: 7.494105784248045\n",
      "total loss: 7.4935363230056655\n",
      "total loss: 7.492966960843334\n",
      "total loss: 7.492397697781689\n",
      "total loss: 7.491828533841122\n",
      "total loss: 7.49125946904179\n",
      "total loss: 7.4906905034035995\n",
      "total loss: 7.490121636946217\n",
      "total loss: 7.489552869689072\n",
      "total loss: 7.488984201651349\n",
      "total loss: 7.488415632852\n",
      "total loss: 7.487847163309735\n",
      "total loss: 7.487278793043023\n",
      "total loss: 7.486710522070108\n",
      "total loss: 7.486142350408988\n",
      "total loss: 7.485574278077431\n",
      "total loss: 7.485006305092973\n",
      "total loss: 7.484438431472916\n",
      "total loss: 7.4838706572343305\n",
      "total loss: 7.483302982394054\n",
      "total loss: 7.482735406968699\n",
      "total loss: 7.482167930974645\n",
      "total loss: 7.481600554428046\n",
      "total loss: 7.481033277344829\n",
      "total loss: 7.480466099740693\n",
      "total loss: 7.479899021631115\n",
      "total loss: 7.479332043031346\n",
      "total loss: 7.478765163956412\n",
      "total loss: 7.47819838442112\n",
      "total loss: 7.477631704440055\n",
      "total loss: 7.47706512402758\n",
      "total loss: 7.476498643197839\n",
      "total loss: 7.47593226196476\n",
      "total loss: 7.475365980342052\n",
      "total loss: 7.474799798343204\n",
      "total loss: 7.474233715981493\n",
      "total loss: 7.473667733269984\n",
      "total loss: 7.47310185022152\n",
      "total loss: 7.472536066848741\n",
      "total loss: 7.471970383164066\n",
      "total loss: 7.47140479917971\n",
      "total loss: 7.470839314907674\n",
      "total loss: 7.470273930359754\n",
      "total loss: 7.469708645547533\n",
      "total loss: 7.4691434604823925\n",
      "total loss: 7.468578375175502\n",
      "total loss: 7.468013389637832\n",
      "total loss: 7.467448503880145\n",
      "total loss: 7.4668837179129985\n",
      "total loss: 7.466319031746754\n",
      "total loss: 7.4657544453915685\n",
      "total loss: 7.465189958857398\n",
      "total loss: 7.464625572153996\n",
      "total loss: 7.464061285290927\n",
      "total loss: 7.46349709827755\n",
      "total loss: 7.46293301112303\n",
      "total loss: 7.462369023836337\n",
      "total loss: 7.461805136426247\n",
      "total loss: 7.4612413489013445\n",
      "total loss: 7.460677661270013\n",
      "total loss: 7.460114073540455\n",
      "total loss: 7.4595505857206765\n",
      "total loss: 7.458987197818494\n",
      "total loss: 7.458423909841541\n",
      "total loss: 7.4578607217972515\n",
      "total loss: 7.4572976336928845\n",
      "total loss: 7.456734645535509\n",
      "total loss: 7.45617175733201\n",
      "total loss: 7.455608969089083\n",
      "total loss: 7.455046280813252\n",
      "total loss: 7.454483692510849\n",
      "total loss: 7.453921204188029\n",
      "total loss: 7.453358815850769\n",
      "total loss: 7.4527965275048595\n",
      "total loss: 7.452234339155924\n",
      "total loss: 7.451672250809398\n",
      "total loss: 7.45111026247055\n",
      "total loss: 7.450548374144465\n",
      "total loss: 7.449986585836066\n",
      "total loss: 7.4494248975500845\n",
      "total loss: 7.448863309291095\n",
      "total loss: 7.448301821063497\n",
      "total loss: 7.447740432871517\n",
      "total loss: 7.447179144719213\n",
      "total loss: 7.446617956610473\n",
      "total loss: 7.446056868549023\n",
      "total loss: 7.4454958805384175\n",
      "total loss: 7.444934992582042\n",
      "total loss: 7.444374204683128\n",
      "total loss: 7.443813516844734\n",
      "total loss: 7.4432529290697556\n",
      "total loss: 7.442692441360934\n",
      "total loss: 7.44213205372084\n",
      "total loss: 7.441571766151894\n",
      "total loss: 7.44101157865635\n",
      "total loss: 7.440451491236305\n",
      "total loss: 7.4398915038937\n",
      "total loss: 7.439331616630324\n",
      "total loss: 7.438771829447799\n",
      "total loss: 7.438212142347606\n",
      "total loss: 7.437652555331058\n",
      "total loss: 7.43709306839933\n",
      "total loss: 7.436533681553437\n",
      "total loss: 7.435974394794243\n",
      "total loss: 7.435415208122463\n",
      "total loss: 7.434856121538667\n",
      "total loss: 7.434297135043269\n",
      "total loss: 7.433738248636544\n",
      "total loss: 7.4331794623186145\n",
      "total loss: 7.4326207760894585\n",
      "total loss: 7.432062189948913\n",
      "total loss: 7.431503703896669\n",
      "total loss: 7.430945317932274\n",
      "total loss: 7.430387032055136\n",
      "total loss: 7.429828846264519\n",
      "total loss: 7.4292707605595485\n",
      "total loss: 7.42871277493921\n",
      "total loss: 7.428154889402351\n",
      "total loss: 7.427597103947685\n",
      "total loss: 7.4270394185737825\n",
      "total loss: 7.42648183327908\n",
      "total loss: 7.425924348061887\n",
      "total loss: 7.425366962920367\n",
      "total loss: 7.424809677852556\n",
      "total loss: 7.424252492856361\n",
      "total loss: 7.423695407929551\n",
      "total loss: 7.42313842306977\n",
      "total loss: 7.422581538274529\n",
      "total loss: 7.422024753541211\n",
      "total loss: 7.421468068867072\n",
      "total loss: 7.420911484249239\n",
      "total loss: 7.420354999684716\n",
      "total loss: 7.419798615170378\n",
      "total loss: 7.419242330702975\n",
      "total loss: 7.4186861462791365\n",
      "total loss: 7.418130061895367\n",
      "total loss: 7.417574077548052\n",
      "total loss: 7.417018193233449\n",
      "total loss: 7.416462408947703\n",
      "total loss: 7.415906724686832\n",
      "total loss: 7.4153511404467425\n",
      "total loss: 7.414795656223217\n",
      "total loss: 7.414240272011922\n",
      "total loss: 7.413684987808414\n",
      "total loss: 7.413129803608124\n",
      "total loss: 7.412574719406373\n",
      "total loss: 7.412019735198373\n",
      "total loss: 7.411464850979214\n",
      "total loss: 7.410910066743879\n",
      "total loss: 7.410355382487239\n",
      "total loss: 7.409800798204054\n",
      "total loss: 7.409246313888976\n",
      "total loss: 7.408691929536541\n",
      "total loss: 7.408137645141185\n",
      "total loss: 7.407583460697232\n",
      "total loss: 7.407029376198903\n",
      "total loss: 7.406475391640307\n",
      "total loss: 7.405921507015457\n",
      "total loss: 7.4053677223182515\n",
      "total loss: 7.404814037542492\n",
      "total loss: 7.404260452681874\n",
      "total loss: 7.403706967729994\n",
      "total loss: 7.403153582680347\n",
      "total loss: 7.402600297526326\n",
      "total loss: 7.402047112261221\n",
      "total loss: 7.401494026878233\n",
      "total loss: 7.400941041370455\n",
      "total loss: 7.400388155730888\n",
      "total loss: 7.399835369952434\n",
      "total loss: 7.399282684027901\n",
      "total loss: 7.3987300979500015\n",
      "total loss: 7.398177611711353\n",
      "total loss: 7.397625225304479\n",
      "total loss: 7.397072938721812\n",
      "total loss: 7.396520751955692\n",
      "total loss: 7.395968664998365\n",
      "total loss: 7.39541667784199\n",
      "total loss: 7.3948647904786355\n",
      "total loss: 7.39431300290028\n",
      "total loss: 7.393761315098814\n",
      "total loss: 7.393209727066041\n",
      "total loss: 7.3926582387936755\n",
      "total loss: 7.39210685027335\n",
      "total loss: 7.391555561496607\n",
      "total loss: 7.391004372454909\n",
      "total loss: 7.39045328313963\n",
      "total loss: 7.389902293542062\n",
      "total loss: 7.389351403653418\n",
      "total loss: 7.388800613464828\n",
      "total loss: 7.3882499229673355\n",
      "total loss: 7.38769933215191\n",
      "total loss: 7.387148841009439\n",
      "total loss: 7.386598449530732\n",
      "total loss: 7.386048157706518\n",
      "total loss: 7.385497965527453\n",
      "total loss: 7.384947872984109\n",
      "total loss: 7.38439788006699\n",
      "total loss: 7.3838479867665185\n",
      "total loss: 7.3832981930730455\n",
      "total loss: 7.382748498976848\n",
      "total loss: 7.382198904468126\n",
      "total loss: 7.381649409537012\n",
      "total loss: 7.381100014173563\n",
      "total loss: 7.380550718367765\n",
      "total loss: 7.3800015221095325\n",
      "total loss: 7.379452425388719\n",
      "total loss: 7.378903428195091\n",
      "total loss: 7.378354530518362\n",
      "total loss: 7.377805732348173\n",
      "total loss: 7.377257033674093\n",
      "total loss: 7.376708434485632\n",
      "total loss: 7.376159934772226\n",
      "total loss: 7.3756115345232525\n",
      "total loss: 7.37506323372802\n",
      "total loss: 7.374515032375774\n",
      "total loss: 7.373966930455697\n",
      "total loss: 7.37341892795691\n",
      "total loss: 7.3728710248684655\n",
      "total loss: 7.372323221179364\n",
      "total loss: 7.371775516878537\n",
      "total loss: 7.371227911954859\n",
      "total loss: 7.3706804063971445\n",
      "total loss: 7.370133000194149\n",
      "total loss: 7.3695856933345665\n",
      "total loss: 7.369038485807041\n",
      "total loss: 7.368491377600147\n",
      "total loss: 7.367944368702412\n",
      "total loss: 7.367397459102304\n",
      "total loss: 7.366850648788235\n",
      "total loss: 7.3663039377485635\n",
      "total loss: 7.36575732597159\n",
      "total loss: 7.365210813445563\n",
      "total loss: 7.364664400158678\n",
      "total loss: 7.36411808609908\n",
      "total loss: 7.363571871254858\n",
      "total loss: 7.363025755614051\n",
      "total loss: 7.362479739164644\n",
      "total loss: 7.361933821894575\n",
      "total loss: 7.361388003791729\n",
      "total loss: 7.360842284843946\n",
      "total loss: 7.360296665039008\n",
      "total loss: 7.359751144364658\n",
      "total loss: 7.359205722808587\n",
      "total loss: 7.358660400358437\n",
      "total loss: 7.358115177001801\n",
      "total loss: 7.357570052726232\n",
      "total loss: 7.357025027519233\n",
      "total loss: 7.356480101368259\n",
      "total loss: 7.355935274260724\n",
      "total loss: 7.355390546183994\n",
      "total loss: 7.354845917125394\n",
      "total loss: 7.3543013870722005\n",
      "total loss: 7.353756956011651\n",
      "total loss: 7.353212623930938\n",
      "total loss: 7.35266839081721\n",
      "total loss: 7.3521242566575795\n",
      "total loss: 7.351580221439105\n",
      "total loss: 7.351036285148817\n",
      "total loss: 7.350492447773702\n",
      "total loss: 7.349948709300696\n",
      "total loss: 7.349405069716709\n",
      "total loss: 7.3488615290086\n",
      "total loss: 7.348318087163197\n",
      "total loss: 7.347774744167282\n",
      "total loss: 7.347231500007605\n",
      "total loss: 7.346688354670871\n",
      "total loss: 7.346145308143753\n",
      "total loss: 7.345602360412883\n",
      "total loss: 7.345059511464856\n",
      "total loss: 7.344516761286234\n",
      "total loss: 7.343974109863535\n",
      "total loss: 7.343431557183247\n",
      "total loss: 7.342889103231821\n",
      "total loss: 7.34234674799567\n",
      "total loss: 7.341804491461175\n",
      "total loss: 7.341262333614676\n",
      "total loss: 7.340720274442487\n",
      "total loss: 7.340178313930879\n",
      "total loss: 7.339636452066095\n",
      "total loss: 7.33909468883434\n",
      "total loss: 7.338553024221789\n",
      "total loss: 7.338011458214579\n",
      "total loss: 7.337469990798818\n",
      "total loss: 7.3369286219605785\n",
      "total loss: 7.336387351685899\n",
      "total loss: 7.335846179960792\n",
      "total loss: 7.335305106771228\n",
      "total loss: 7.334764132103155\n",
      "total loss: 7.334223255942483\n",
      "total loss: 7.333682478275089\n",
      "total loss: 7.333141799086825\n",
      "total loss: 7.332601218363506\n",
      "total loss: 7.33206073609092\n",
      "total loss: 7.331520352254819\n",
      "total loss: 7.3309800668409295\n",
      "total loss: 7.3304398798349455\n",
      "total loss: 7.329899791222527\n",
      "total loss: 7.329359800989309\n",
      "total loss: 7.32881990912089\n",
      "total loss: 7.328280115602843\n",
      "total loss: 7.327740420420712\n",
      "total loss: 7.327200823560009\n",
      "total loss: 7.326661325006214\n",
      "total loss: 7.326121924744782\n",
      "total loss: 7.325582622761134\n",
      "total loss: 7.325043419040663\n",
      "total loss: 7.324504313568735\n",
      "total loss: 7.323965306330682\n",
      "total loss: 7.323426397311811\n",
      "total loss: 7.322887586497401\n",
      "total loss: 7.322348873872692\n",
      "total loss: 7.32181025942291\n",
      "total loss: 7.321271743133236\n",
      "total loss: 7.3207333249888356\n",
      "total loss: 7.320195004974838\n",
      "total loss: 7.319656783076345\n",
      "total loss: 7.3191186592784305\n",
      "total loss: 7.318580633566133\n",
      "total loss: 7.3180427059244755\n",
      "total loss: 7.3175048763384405\n",
      "total loss: 7.3169671447929865\n",
      "total loss: 7.316429511273041\n",
      "total loss: 7.315891975763507\n",
      "total loss: 7.315354538249252\n",
      "total loss: 7.314817198715119\n",
      "total loss: 7.314279957145921\n",
      "total loss: 7.313742813526443\n",
      "total loss: 7.313205767841441\n",
      "total loss: 7.31266882007564\n",
      "total loss: 7.312131970213735\n",
      "total loss: 7.3115952182404\n",
      "total loss: 7.311058564140268\n",
      "total loss: 7.310522007897951\n",
      "total loss: 7.309985549498032\n",
      "total loss: 7.309449188925061\n",
      "total loss: 7.308912926163557\n",
      "total loss: 7.308376761198017\n",
      "total loss: 7.307840694012902\n",
      "total loss: 7.307304724592646\n",
      "total loss: 7.306768852921652\n",
      "total loss: 7.306233078984296\n",
      "total loss: 7.305697402764922\n",
      "total loss: 7.305161824247843\n",
      "total loss: 7.304626343417347\n",
      "total loss: 7.304090960257684\n",
      "total loss: 7.303555674753082\n",
      "total loss: 7.303020486887734\n",
      "total loss: 7.302485396645805\n",
      "total loss: 7.301950404011426\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/4 [00:05<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total loss: 7.301415508968701\n",
      "total loss: 7.300880711501703\n",
      "total loss: 7.300346011594475\n",
      "total loss: 7.2998114092310225\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 27\u001b[0m\n\u001b[1;32m     24\u001b[0m dag_fn \u001b[38;5;241m=\u001b[39m flex\u001b[38;5;241m.\u001b[39mLogDet()\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# Learn the DAG\u001b[39;00m\n\u001b[0;32m---> 27\u001b[0m W_est \u001b[38;5;241m=\u001b[39m \u001b[43mflex\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstruct_learn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnonlinear_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconstrained_solver\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcons_solver\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[43m    \u001b[49m\u001b[43munconstrained_solver\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muncons_solver\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[43m    \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdag_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdag_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[43m    \u001b[49m\u001b[43mw_threshold\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.3\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     37\u001b[0m acc \u001b[38;5;241m=\u001b[39m utils\u001b[38;5;241m.\u001b[39mcount_accuracy(nonlinear_B_true, W_est \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mResults: \u001b[39m\u001b[38;5;124m\"\u001b[39m, acc)\n",
      "File \u001b[0;32m~/dagrad/dagrad/flex/struct_learn.py:84\u001b[0m, in \u001b[0;36mstruct_learn\u001b[0;34m(dataset, model, constrained_solver, unconstrained_solver, loss_fn, dag_fn, w_threshold, device, dtype, verbose, suppress_warnings)\u001b[0m\n\u001b[1;32m     81\u001b[0m unconstrained_solver\u001b[38;5;241m.\u001b[39mvwarn \u001b[38;5;241m=\u001b[39m vwarn \n\u001b[1;32m     83\u001b[0m time_start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()    \n\u001b[0;32m---> 84\u001b[0m \u001b[43mconstrained_solver\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43munconstrained_solver\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdag_fn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTotal Time: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtime\u001b[38;5;241m.\u001b[39mtime()\u001b[38;5;250m \u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;250m \u001b[39mtime_start\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     87\u001b[0m W_est \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39madj()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n",
      "File \u001b[0;32m~/dagrad/dagrad/flex/modules/constrained_solvers.py:17\u001b[0m, in \u001b[0;36mConstrainedSolver.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds):\n\u001b[0;32m---> 17\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msolve\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/dagrad/dagrad/flex/modules/constrained_solvers.py:116\u001b[0m, in \u001b[0;36mPathFollowing.solve\u001b[0;34m(self, dataset, model, unconstrained_solver, loss_fn, dag_fn)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvprint(\n\u001b[1;32m    113\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mSolving unconstrained problem #\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m with mu=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmu\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, s=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdag_fn\u001b[38;5;241m.\u001b[39ms\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_steps[i]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m steps\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    114\u001b[0m )\n\u001b[1;32m    115\u001b[0m unconstrained_solver\u001b[38;5;241m.\u001b[39mnum_steps \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_steps[i]\n\u001b[0;32m--> 116\u001b[0m success \u001b[38;5;241m=\u001b[39m \u001b[43munconstrained_solver\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    117\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnew_loss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdag_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr_scale\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr_scheduler\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m success \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[1;32m    121\u001b[0m     model\u001b[38;5;241m.\u001b[39mload_state_dict(\n\u001b[1;32m    122\u001b[0m         model_copy\u001b[38;5;241m.\u001b[39mstate_dict()\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m    123\u001b[0m     )  \u001b[38;5;66;03m# .to(device)?\u001b[39;00m\n",
      "File \u001b[0;32m~/dagrad/dagrad/flex/modules/unconstrained_solvers.py:15\u001b[0m, in \u001b[0;36mUnconstrainedSolver.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds):\n\u001b[0;32m---> 15\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msolve\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/dagrad/dagrad/flex/modules/unconstrained_solvers.py:62\u001b[0m, in \u001b[0;36mGradientBasedSolver.solve\u001b[0;34m(self, dataset, model, loss, dag_fn, lr_scale, lr_scheduler)\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvprint(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound h negative \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mh\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m at iter \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Stopping.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m---> 62\u001b[0m obj_new \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m lr_scheduler \u001b[38;5;129;01mand\u001b[39;00m (i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m1000\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:  \u001b[38;5;66;03m# every 1000 iters reduce lr\u001b[39;00m\n\u001b[1;32m     64\u001b[0m     scheduler\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/optim/optimizer.py:385\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    380\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    381\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    382\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    383\u001b[0m             )\n\u001b[0;32m--> 385\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    386\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    388\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/optim/optimizer.py:76\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     74\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     75\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[0;32m---> 76\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     78\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/optim/adam.py:146\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m closure \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    145\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39menable_grad():\n\u001b[0;32m--> 146\u001b[0m         loss \u001b[38;5;241m=\u001b[39m \u001b[43mclosure\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    148\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m group \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparam_groups:\n\u001b[1;32m    149\u001b[0m     params_with_grad \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/dagrad/dagrad/flex/modules/unconstrained_solvers.py:52\u001b[0m, in \u001b[0;36mGradientBasedSolver.solve.<locals>.closure\u001b[0;34m()\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     51\u001b[0m obj \u001b[38;5;241m=\u001b[39m loss(model(dataset), dataset)\n\u001b[0;32m---> 52\u001b[0m \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m obj\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    521\u001b[0m     )\n\u001b[0;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/autograd/__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/dagrad/dagrad/flex/modules/models.py:162\u001b[0m, in \u001b[0;36mMLP.make_hook_function.<locals>.hook_function\u001b[0;34m(grad)\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmake_hook_function\u001b[39m(d):\n\u001b[0;32m--> 162\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mhook_function\u001b[39m(grad):\n\u001b[1;32m    163\u001b[0m         grad_clone \u001b[38;5;241m=\u001b[39m grad\u001b[38;5;241m.\u001b[39mclone()\n\u001b[1;32m    164\u001b[0m         grad_clone \u001b[38;5;241m=\u001b[39m grad_clone\u001b[38;5;241m.\u001b[39mview(d, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, d)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# nonlinear model\n",
    "model = flex.MLP(dims=[d, 10, 1], activation=\"sigmoid\", bias=True)\n",
    "\n",
    "# Use path following to solve the constrained problem\n",
    "cons_solver = flex.PathFollowing(\n",
    "    num_iter=4,\n",
    "    mu_init=0.1,\n",
    "    mu_scale=0.1,\n",
    "    logdet_coeff=1.0,\n",
    "    num_steps=[5e4, 8e4],\n",
    "    weight_decay=0.02,\n",
    "    l1_coeff=0.005,\n",
    ")\n",
    "\n",
    "# use Adam to solve the unconstrained problem\n",
    "uncons_solver = flex.GradientBasedSolver(\n",
    "    optimizer=torch.optim.Adam(model.parameters(), lr=2e-4, betas=(0.99, 0.999))\n",
    ")\n",
    "\n",
    "# Use NLL loss\n",
    "loss_fn = flex.NLLLoss()\n",
    "\n",
    "# Use LogDet as DAG function\n",
    "dag_fn = flex.LogDet()\n",
    "\n",
    "# Learn the DAG\n",
    "W_est = flex.struct_learn(\n",
    "    dataset=nonlinear_dataset,\n",
    "    model=model,\n",
    "    constrained_solver=cons_solver,\n",
    "    unconstrained_solver=uncons_solver,\n",
    "    loss_fn=loss_fn,\n",
    "    dag_fn=dag_fn,\n",
    "    w_threshold=0.3,\n",
    ")\n",
    "\n",
    "acc = utils.count_accuracy(nonlinear_B_true, W_est != 0)\n",
    "print(\"Results: \", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
