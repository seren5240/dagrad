{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No GPU automatically detected. Setting SETTINGS.GPU to 0, and SETTINGS.NJOBS to cpu_count.\n",
      "IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x11d2fdb70>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "import torch\n",
    "import dagrad.flex as flex\n",
    "from dagrad.utils import utils\n",
    "import numpy as np\n",
    "\n",
    "utils.set_random_seed(1)\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def postprocess(B, graph_thres=0.3):\n",
    "    \"\"\"Post-process estimated solution:\n",
    "        (1) Thresholding.\n",
    "        (2) Remove the edges with smallest absolute weight until a DAG\n",
    "            is obtained.\n",
    "\n",
    "    Args:\n",
    "        B (numpy.ndarray): [d, d] weighted matrix.\n",
    "        graph_thres (float): Threshold for weighted matrix. Default: 0.3.\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: [d, d] weighted matrix of DAG.\n",
    "    \"\"\"\n",
    "    B = np.copy(B)\n",
    "    B[np.abs(B) <= graph_thres] = 0    # Thresholding\n",
    "    B, _ = utils.threshold_till_dag(B)\n",
    "\n",
    "    return B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Struct Learn via Flex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`flex.struct_learn` is a function that requires the following 5 inputs:\n",
    "- `dataset`: A data matrix with `n x d` shape, where `n` is the number of samples and `d` is the number of variables/features.\n",
    "\n",
    "- `model`: The SEM module. We provide standard SEM implementations such as---LinearModel, LogisticModel, MLP\n",
    "\n",
    "- `constrained_solver`: An instance of a ConstrainedSolver class. We provide implementations such as `PathFollowing` and `AugmentedLagrangian`\n",
    "\n",
    "- `unconstrained_solver`: An instance of an UnconstrainedSolver class. We provide implementation for `GradientBasedSolver`\n",
    "\n",
    "- `loss_fn`: Instance of a Loss class. All available losses can be found at flex/loss.py\n",
    "\n",
    "- `dag_fn`: Instance of a DagFn class. All available DAG functions can be found at flex/dags.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate linear data\n",
    "n, d, s0, graph_type, sem_type = 1000, 20, 20, \"ER\", \"gauss\"\n",
    "linear_B_true = utils.simulate_dag(d, s0, graph_type)\n",
    "linear_dataset = utils.simulate_linear_sem(linear_B_true, n, sem_type)\n",
    "\n",
    "# Generate non-linear data\n",
    "n, d, s0, graph_type, sem_type = 1000, 5, 5, \"ER\", \"mlp\"\n",
    "nonlinear_B_true = utils.simulate_dag(d, s0, graph_type)\n",
    "nonlinear_dataset = utils.simulate_nonlinear_sem(nonlinear_B_true, n, sem_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using flex to implement NOTEARS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## linear NOTEARS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:37<00:00,  3.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Time: 37.8957781791687\n",
      "Results:  {'fdr': 0.0, 'tpr': 1.0, 'fpr': 0.0, 'shd': 0, 'nnz': 20}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Linear model\n",
    "model = flex.LinearModel(d)\n",
    "\n",
    "# Use AML to solve the constrained problem\n",
    "cons_solver = flex.AugmentedLagrangian(\n",
    "    num_iter=10,\n",
    "    num_steps=[3e4,6e4],\n",
    "    l1_coeff=0.03,\n",
    ")\n",
    "\n",
    "# Use Adam to solve the unconstrained problem\n",
    "uncons_solver = flex.GradientBasedSolver(\n",
    "    optimizer=torch.optim.Adam(model.parameters(), lr=3e-4),\n",
    ")\n",
    "\n",
    "# Use MSE loss\n",
    "loss_fn = flex.MSELoss()\n",
    "\n",
    "# Use Trace of matrix exponential as DAG function\n",
    "dag_fn = flex.Exp()\n",
    "\n",
    "# Learn the DAG\n",
    "W_est = flex.struct_learn(\n",
    "    dataset=linear_dataset,\n",
    "    model=model,\n",
    "    constrained_solver=cons_solver,\n",
    "    unconstrained_solver=uncons_solver,\n",
    "    loss_fn=loss_fn,\n",
    "    dag_fn=dag_fn,\n",
    "    w_threshold=0.3,\n",
    ")\n",
    "\n",
    "acc = utils.count_accuracy(linear_B_true, W_est != 0)\n",
    "print(\"Results: \", acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## nonlinear NOTEARS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dcdi_aug_lagrangian(n, d, s0, num_layers=2, noise_type=\"gauss\"):\n",
    "    graph_type, sem_type = \"ER\", \"mlp\"\n",
    "    nonlinear_B_true = utils.simulate_dag(d, s0, graph_type)\n",
    "    nonlinear_dataset = utils.simulate_nonlinear_sem(nonlinear_B_true, n, sem_type, noise_type=noise_type)\n",
    "\n",
    "    # Nonlinear model\n",
    "    model = flex.MLP(dims=[d, 1, d], num_layers=num_layers, hid_dim=16, activation=\"relu\", bias=True)\n",
    "\n",
    "    # Use AML to solve the constrained problem\n",
    "    cons_solver = flex.AugmentedLagrangian(\n",
    "        num_iter=1000000,\n",
    "        num_steps=[1,1],\n",
    "        l1_coeff=0.1,\n",
    "        # weight_decay=0.01,\n",
    "        rho_init=1e-8,\n",
    "        rho_scale=2,\n",
    "    )\n",
    "\n",
    "    # Use Adam to solve the unconstrained problem\n",
    "    uncons_solver = flex.GradientBasedSolver(\n",
    "        optimizer=torch.optim.RMSprop(model.parameters(), lr=1e-3),\n",
    "    )\n",
    "\n",
    "    # Use MSE loss\n",
    "    loss_fn = flex.MSELoss()\n",
    "\n",
    "    dag_fn = flex.DCDI_h()\n",
    "\n",
    "    # Learn the DAG\n",
    "    W_est = flex.struct_learn(\n",
    "        dataset=nonlinear_dataset,\n",
    "        model=model,\n",
    "        constrained_solver=cons_solver,\n",
    "        unconstrained_solver=uncons_solver,\n",
    "        loss_fn=loss_fn,\n",
    "        dag_fn=dag_fn,\n",
    "        w_threshold=0.0,\n",
    "    )\n",
    "\n",
    "    # W_est = postprocess(W_est)\n",
    "\n",
    "    acc = utils.count_accuracy(nonlinear_B_true, W_est != 0)\n",
    "    print(\"Results: \", acc)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Time: 144.20864176750183\n",
      "struct learn returning [[0. 0. 1. 1. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "Results:  {'fdr': 0.5, 'tpr': 0.4, 'fpr': 0.4, 'shd': 4, 'sid': 7.0, 'nnz': 4}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'fdr': 0.5, 'tpr': 0.4, 'fpr': 0.4, 'shd': 4, 'sid': 7.0, 'nnz': 4}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dcdi_aug_lagrangian(1000, 5, 5, num_layers=2, noise_type=\"gauss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shds = []\n",
    "for i in range(10):\n",
    "    acc = dcdi_aug_lagrangian(1000, 5, 5, num_layers=2, noise_type=\"gauss\")\n",
    "    shds.append(acc['shd'])\n",
    "print(\"Average SHD: \", np.mean(shds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using flex to implement DAGMA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear DAGMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/seren/dagrad/dagrad/flex/modules/constrained_solvers.py:49: UserWarning: Using the first value from num_steps for the first 3 iterations\n",
      "  self.vwarn(\n",
      "100%|██████████| 5/5 [00:08<00:00,  1.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Time: 8.867684841156006\n",
      "Results:  {'fdr': 0.0, 'tpr': 1.0, 'fpr': 0.0, 'shd': 0, 'nnz': 20}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Linear model\n",
    "model = flex.LinearModel(d)\n",
    "\n",
    "# Use path following to solve the constrained problem\n",
    "cons_solver = flex.PathFollowing(\n",
    "    num_iter=5,\n",
    "    mu_init=1.0,\n",
    "    mu_scale=0.1,\n",
    "    logdet_coeff=[1.0, .9, .8, .7, .6],\n",
    "    num_steps=[3e4, 6e4],\n",
    "    l1_coeff=0.03,\n",
    ")\n",
    "\n",
    "# use Adam to solve the unconstrained problem\n",
    "uncons_solver = flex.GradientBasedSolver(\n",
    "    optimizer=torch.optim.Adam(model.parameters(), lr=3e-4, betas=(0.99, 0.999))\n",
    ")\n",
    "\n",
    "# Use MSE loss\n",
    "loss_fn = flex.MSELoss()\n",
    "\n",
    "# Use LogDet as DAG function\n",
    "dag_fn = flex.LogDet()\n",
    "\n",
    "W_est = flex.struct_learn(\n",
    "    dataset=linear_dataset,\n",
    "    model=model,\n",
    "    constrained_solver=cons_solver,\n",
    "    unconstrained_solver=uncons_solver,\n",
    "    loss_fn=loss_fn,\n",
    "    dag_fn=dag_fn,\n",
    "    w_threshold=0.3,\n",
    ")\n",
    "\n",
    "acc = utils.count_accuracy(linear_B_true, W_est != 0)\n",
    "print(\"Results: \", acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nonlinear DAGMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/seren/dagrad/dagrad/flex/modules/constrained_solvers.py:51: UserWarning: Using the first value from num_steps for the first 2 iterations\n",
      "  self.vwarn(\n",
      "100%|██████████| 4/4 [09:40<00:00, 145.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Time: 580.489175081253\n",
      "Results:  {'fdr': 0.0, 'tpr': 0.0, 'fpr': 0.0, 'shd': 5, 'nnz': 0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "n, d, s0, graph_type, sem_type = 1000, 5, 5, \"ER\", \"mlp\"\n",
    "nonlinear_B_true = utils.simulate_dag(d, s0, graph_type)\n",
    "nonlinear_dataset = utils.simulate_nonlinear_sem(nonlinear_B_true, n, sem_type)\n",
    "\n",
    "# nonlinear model\n",
    "model = flex.MLP(dims=[d, 2, 5], num_layers=2, hid_dim=10, activation=\"sigmoid\", bias=True)\n",
    "\n",
    "# Use path following to solve the constrained problem\n",
    "cons_solver = flex.PathFollowing(\n",
    "    num_iter=4,\n",
    "    mu_init=0.1,\n",
    "    mu_scale=0.1,\n",
    "    logdet_coeff=1.0,\n",
    "    num_steps=[5e4, 8e4],\n",
    "    weight_decay=0.02,\n",
    "    l1_coeff=0.005,\n",
    ")\n",
    "\n",
    "# use Adam to solve the unconstrained problem\n",
    "uncons_solver = flex.GradientBasedSolver(\n",
    "    # optimizer=torch.optim.Adam(model.parameters(), lr=2e-4, betas=(0.99, 0.999))\n",
    "    optimizer=torch.optim.SGD(model.parameters(), lr=2e-4)\n",
    ")\n",
    "\n",
    "# Use NLL loss\n",
    "loss_fn = flex.NLLLoss()\n",
    "\n",
    "# Use LogDet as DAG function\n",
    "dag_fn = flex.LogDet()\n",
    "\n",
    "# Learn the DAG\n",
    "W_est = flex.struct_learn(\n",
    "    dataset=nonlinear_dataset,\n",
    "    model=model,\n",
    "    constrained_solver=cons_solver,\n",
    "    unconstrained_solver=uncons_solver,\n",
    "    loss_fn=loss_fn,\n",
    "    dag_fn=dag_fn,\n",
    "    w_threshold=0.3,\n",
    ")\n",
    "\n",
    "W_est = postprocess(W_est)\n",
    "acc = utils.count_accuracy(nonlinear_B_true, W_est != 0)\n",
    "print(\"Results: \", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
