{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import packages "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all the necessary modules\n",
    "from dagrad import dagrad # dagrad is the main class for learning the structure of a DAG\n",
    "from dagrad import generate_linear_data, generate_nonlinear_data, count_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Original NOTEARS Implementation for Linear Model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/duntrain/anaconda3/envs/test_dagrad/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Original DAGMA Implementation for Linear Model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 180000/180000.0 [00:01<00:00, 170229.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter is automatically set up.\n",
      " size_small: 15, size_large: 28, no_large_search: 1\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Unknown combination of loss_fn and reg: user_loss, none. This combination is not supported. You can use other optimizers.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m W_notears \u001b[38;5;241m=\u001b[39m dagrad(X, model \u001b[38;5;241m=\u001b[39m model, method \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnotears\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;66;03m# Learn the structure of the DAG using Notears\u001b[39;00m\n\u001b[1;32m      5\u001b[0m W_dagma \u001b[38;5;241m=\u001b[39m dagrad(X, model \u001b[38;5;241m=\u001b[39m model, method \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdagma\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;66;03m# Learn the structure of the DAG using Dagma\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m W_topo \u001b[38;5;241m=\u001b[39m \u001b[43mdagrad\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtopo\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43muser_loss\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# Learn the structure of the DAG using Topo\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLinear Model\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata size: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, graph type: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgraph_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, sem type: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msem_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/dagrad/dagrad/core.py:234\u001b[0m, in \u001b[0;36mdagrad\u001b[0;34m(X, method, model, loss_fn, h_fn, reg, optimizer, compute_lib, device, general_options, method_options, optimizer_options)\u001b[0m\n\u001b[1;32m    232\u001b[0m             W_est \u001b[38;5;241m=\u001b[39m parameter_tuning(dag_learning_func \u001b[38;5;241m=\u001b[39m topo_linear, X \u001b[38;5;241m=\u001b[39m X, model \u001b[38;5;241m=\u001b[39m model, loss_fn \u001b[38;5;241m=\u001b[39m loss_fn, h_fn \u001b[38;5;241m=\u001b[39m h_fn, reg \u001b[38;5;241m=\u001b[39m reg, optimizer \u001b[38;5;241m=\u001b[39m optimizer, options \u001b[38;5;241m=\u001b[39m options)\n\u001b[1;32m    233\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 234\u001b[0m             W_est \u001b[38;5;241m=\u001b[39m \u001b[43mtopo_linear\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[43mh_fn\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mh_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreg\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mreg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    236\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m compute_lib \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtorch\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    237\u001b[0m     options[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdevice\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m device\n",
      "File \u001b[0;32m~/dagrad/dagrad/method/topo.py:144\u001b[0m, in \u001b[0;36mtopo_linear\u001b[0;34m(X, topo, loss_fn, h_fn, reg, optimizer, no_large_search, size_small, size_large, dtype, verbose, **options)\u001b[0m\n\u001b[1;32m    142\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUnknown regularizer: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mreg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    143\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 144\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUnknown combination of loss_fn and reg: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss_fn\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mreg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. This combination is not supported. You can use other optimizers.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    146\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_regress\u001b[39m(X,y):\n",
      "\u001b[0;31mValueError\u001b[0m: Unknown combination of loss_fn and reg: user_loss, none. This combination is not supported. You can use other optimizers."
     ]
    }
   ],
   "source": [
    "n, d, s0, graph_type, sem_type = 1000, 10, 10, 'ER', 'gauss' # Define the parameters of the data\n",
    "X, W_true, B_true = generate_linear_data(n,d,s0,graph_type,sem_type,seed  =2) # Generate the data\n",
    "model = 'linear' # Define the model\n",
    "W_notears = dagrad(X, model = model, method = 'notears') # Learn the structure of the DAG using Notears\n",
    "W_dagma = dagrad(X, model = model, method = 'dagma') # Learn the structure of the DAG using Dagma\n",
    "W_topo = dagrad(X, model = model, method = 'topo', loss_fn='user_loss') # Learn the structure of the DAG using Topo\n",
    "print(f\"Linear Model\")\n",
    "print(f\"data size: {n}, graph type: {graph_type}, sem type: {sem_type}\")\n",
    "\n",
    "acc_notears = count_accuracy(B_true, W_notears != 0) # Measure the accuracy of the learned structure using Notears\n",
    "print('Accuracy of Notears:', acc_notears)\n",
    "\n",
    "acc_dagma = count_accuracy(B_true, W_dagma != 0) # Measure the accuracy of the learned structure using Dagma\n",
    "print('Accuracy of Dagma:', acc_dagma)\n",
    "\n",
    "acc_topo = count_accuracy(B_true, W_topo != 0) # Measure the accuracy of the learned structure using Topo\n",
    "print('Accuracy of Topo:', acc_topo)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working with 4 fold cross validation on l1 with lambda1s: [0, 0.05, 0.1, 0.2, 0.5]\n",
      "Optimal lambda1: 0.05\n",
      "Retrain the model with optimal lambda1\n",
      "Model retrained with optimal lambda1\n",
      "{'fdr': 0.0, 'tpr': 1.0, 'fpr': 0.0, 'shd': 0, 'nnz': 10}\n"
     ]
    }
   ],
   "source": [
    "W_est = dagrad(X = X, \n",
    "               method = 'notears', \n",
    "               reg = 'l1', \n",
    "               optimizer = 'lbfgs', \n",
    "               general_options = {'tuning_method':'cv',\n",
    "                                  'K':4, \n",
    "                                  'reg_paras':[0, 0.05, 0.1, 0.2, 0.5]})\n",
    "acc = count_accuracy(B_true, W_est != 0)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Model with different options (NOTEARS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Notears: {'fdr': 0.0, 'tpr': 0.0, 'fpr': 0.0, 'shd': 10, 'nnz': 0}\n"
     ]
    }
   ],
   "source": [
    "n, d, s0, graph_type, sem_type = 1000, 10, 10, 'ER', 'gauss' # Define the parameters of the data\n",
    "X, W_true, B_true = generate_linear_data(n,d,s0,graph_type,sem_type) # Generate the data\n",
    "model = 'linear' # Define the model\n",
    "\n",
    "general_options = {'gamma':0.4, 'lambda1':0.1} # Define the general options\n",
    "optimizer_options = {'lr':0.01,'num_steps':5000, 'check_iterate':500, 'tol':1e-5} # Define the optimizer options\n",
    "method_options = {'verbose': False, 'rho':0.1} # Define the method options\n",
    "# Learn the structure of the DAG using Notears\n",
    "W_notears = dagrad(X, model = model, \n",
    "                   reg = 'mcp', \n",
    "                   optimizer= 'adam', \n",
    "                   method = 'notears', \n",
    "                   compute_lib='numpy',\n",
    "                   general_options = general_options, \n",
    "                   optimizer_options = optimizer_options) \n",
    "acc_notears = count_accuracy(B_true, W_notears != 0) # Measure the accuracy of the learned structure using Notears\n",
    "print('Accuracy of Notears:', acc_notears)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/duntrain/Documents/Research/Code/Python/DAGlearner/dagrad/daglearner.py:162: UserWarning: For l2 or logistic loss, TOPO would be faster using optimizer: sklearn. But you are currently using optimizer: lbfgs.\n",
      "  warn(f\"For l2 or logistic loss, TOPO would be faster using optimizer: sklearn. But you are currently using optimizer: {optimizer}.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter is automatically set up.\n",
      " size_small: 10, size_large: 40, no_large_search: 3\n",
      "Total Time: 3.2617831230163574\n",
      "Accuracy of Topo: {'fdr': 0.0, 'tpr': 1.0, 'fpr': 0.0, 'shd': 0, 'nnz': 10}\n"
     ]
    }
   ],
   "source": [
    "n, d, s0, graph_type, sem_type = 1000, 10, 10, 'ER', 'gauss' # Define the parameters of the data\n",
    "X, W_true, B_true = generate_linear_data(n,d,s0,graph_type,sem_type) # Generate the data\n",
    "model = 'linear' # Define the model\n",
    "topo_init = list(range(d))\n",
    "\n",
    "general_options = {'gamma':0.4, 'lambda1':0.1} # Define the general options\n",
    "method_options = {'no_large_search':3, 'size_small': 10, 'size_large': 40,'topo':topo_init} # Define the method options\n",
    "\n",
    "W_topo = dagrad(X, model = model, method = 'topo', loss_fn= 'l2', reg = 'mcp', optimizer = 'lbfgs', general_options=general_options, method_options=method_options) # Learn the structure of the DAG using Topo\n",
    "acc_topo = count_accuracy(B_true, W_topo != 0) # Measure the accuracy of the learned structure using Topo\n",
    "print('Accuracy of Topo:', acc_topo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nonlinear Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nonlinear Model\n",
      "data size: 1000, graph type: ER, sem type: mlp\n",
      "Accuracy of Notears: {'fdr': 0.0, 'tpr': 1.0, 'fpr': 0.0, 'shd': 0, 'nnz': 10}\n",
      "Accuracy of Dagma: {'fdr': 0.09090909090909091, 'tpr': 1.0, 'fpr': 0.02857142857142857, 'shd': 1, 'nnz': 11}\n",
      "Accuracy of Topo: {'fdr': 0.5833333333333334, 'tpr': 1.0, 'fpr': 0.4, 'shd': 14, 'nnz': 24}\n"
     ]
    }
   ],
   "source": [
    "n, d, s0, graph_type, sem_type = 1000, 10, 10, 'ER', 'mlp' # Define the parameters of the data\n",
    "X, B_true = generate_nonlinear_data(n,d,s0,graph_type,sem_type,seed= 3) # Generate the data\n",
    "model = 'nonlinear' # Define the model\n",
    "W_notears = dagrad(X, model = model, method = 'notears') # Learn the structure of the DAG using Notears\n",
    "W_dagma = dagrad(X, model = model, method = 'dagma') # Learn the structure of the DAG using Dagma\n",
    "W_topo = dagrad(X, model = model, method = 'topo', general_options = {'lambda1':0.003,'lambda2':0}) # Learn the structure of the DAG using Topo\n",
    "print(f'Nonlinear Model')\n",
    "print(f\"data size: {n}, graph type: {graph_type}, sem type: {sem_type}\")\n",
    "\n",
    "acc_notears = count_accuracy(B_true, W_notears != 0) # Measure the accuracy of the learned structure using Notears\n",
    "print('Accuracy of Notears:', acc_notears)\n",
    "\n",
    "acc_dagma = count_accuracy(B_true, W_dagma != 0) # Measure the accuracy of the learned structure using Dagma\n",
    "print('Accuracy of Dagma:', acc_dagma)\n",
    "\n",
    "acc_topo = count_accuracy(B_true, W_topo != 0) # Measure the accuracy of the learned structure using Topo\n",
    "print('Accuracy of Topo:', acc_topo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Topo: {'fdr': 0.2, 'tpr': 0.8, 'fpr': 0.05714285714285714, 'shd': 4, 'nnz': 10}\n"
     ]
    }
   ],
   "source": [
    "W_topo = dagrad(X, model = model, method = 'topo', general_options = {'lambda1':0.003,'lambda2':0}) # Measure the accuracy of the learned structure using Topo\n",
    "acc_topo = count_accuracy(B_true, W_topo != 0) # Measure the accuracy of the learned structure using Topo\n",
    "print('Accuracy of Topo:', acc_topo)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test_dagrad",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
